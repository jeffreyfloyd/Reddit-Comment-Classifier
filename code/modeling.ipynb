{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d859ebc-82fe-4ccb-805e-017ae74ac86b",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Now that we've taken a look as some of the more interesting features of our data and also engineered a few along the way lets see if we can get some machine learning going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c1d4e7-a100-42c7-99f2-c0b77c62d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xg\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e61d90c-797a-464e-aa6b-729c92ab118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>neg_sent</th>\n",
       "      <th>neu_sent</th>\n",
       "      <th>pos_sent</th>\n",
       "      <th>comp_sent</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What’s a ghost profile</td>\n",
       "      <td>4</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i’m bummed for sure, but i’ll improvise someth...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Makes my mouth water.</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2, 3, and 4 are all the same car. 2 has sunroo...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.2755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i wish there were a \"love\" button. damn!!! bea...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  word_count  neg_sent  \\\n",
       "0                             What’s a ghost profile           4     0.535   \n",
       "1  i’m bummed for sure, but i’ll improvise someth...          20     0.000   \n",
       "2                              Makes my mouth water.           4     0.000   \n",
       "3  2, 3, and 4 are all the same car. 2 has sunroo...          35     0.070   \n",
       "4  i wish there were a \"love\" button. damn!!! bea...          18     0.120   \n",
       "\n",
       "   neu_sent  pos_sent  comp_sent  subreddit  \n",
       "0     0.465      0.00    -0.3182          0  \n",
       "1     1.000      0.00     0.0000          0  \n",
       "2     1.000      0.00     0.0000          1  \n",
       "3     0.930      0.00    -0.2755          0  \n",
       "4     0.489      0.39     0.7249          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/reddit_comments_partial.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2d7e3d-8eba-40de-95f5-652c02e15887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment       0\n",
       "word_count    0\n",
       "neg_sent      0\n",
       "neu_sent      0\n",
       "pos_sent      0\n",
       "comp_sent     0\n",
       "subreddit     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to make sure we didnt get any null values once more\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f757a2e-eb11-4864-870b-db9055f717c5",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "Let's see the exact spread of our final data to get a good idea of our baseline score we need to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd3972c-0c80-4291-a9f4-6a55b9f1c5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.500994\n",
       "1    0.499006\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22784a3f-5bf9-42a5-b945-13d96c42f167",
   "metadata": {},
   "source": [
    "Our metric to beat here is 50%. We'll test against this for accuracy but keep an eye on our f1 scores as well to make sure we're at least somewhat balanced on our misclassifications. Quick reminder that oddlysatisfying is our positive class (1) and mildlyinfuriating is our negative class (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c749da-e76b-423f-b388-e9660857bd1e",
   "metadata": {},
   "source": [
    "## Additional Transformations\n",
    "One final thing I would like to add in here is a means of removing puncuation to reduce dimensionality for the sake of model run times. We'll use a regular expression tokenizer to help us out here. We will also have to redo our CountVectorizer transformation as it caused storage issues previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f66ee7-14be-4528-ba0a-eace3afc5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# clean all puncuation out of our comments\n",
    "data['comment'] = [' '.join(tokenizer.tokenize(str(value).lower())) for value in data['comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906e94bb-b86e-436b-a21d-1ebc9cb639ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words we determined from our cleaning step\n",
    "stops = ['like', 'just', 'don', 'know', 'think', 'time', 'people', 'looks', 'good', 'really', 'make', 've', 'way', 'want', 'lol', 'thing', 'did', 'work', 'right',\n",
    "         'need', 'use', 'look', 'does', 'water', 'got', 'thought', 'used', 'yeah', 'going', 'shit', 'pretty', 'say', 'actually', 'probably', 'sure', 'didn', 'll',\n",
    "         'doesn', 'little', 'makes', 'lot', 'day', 'yes', 'years', 'things', 'better', 'oh', 'isn', 'feel', 'doing', 'long', 'man', 'stuff', 'fuck', 'different',\n",
    "         'maybe', 'mean', 'gt', 'new', 'bad', 'getting', 'said', 'job', 'fucking', 'life', 'point', 'old', 'post', 'car', 'eat', 'person', 'house', 'wrong', 'big',\n",
    "         'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
    "         'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what',\n",
    "         'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
    "         'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "         'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "         'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
    "         'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll',\n",
    "         'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "         \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\",\n",
    "         'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e3e37d-6dec-4f2c-b2ec-570d5a0e411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zikr</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>word_count</th>\n",
       "      <th>neg_sent</th>\n",
       "      <th>neu_sent</th>\n",
       "      <th>pos_sent</th>\n",
       "      <th>comp_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.2755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.7249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  01  02  03  04  05  07  08  10  ...  zeros  zikr  zone  zoomed  \\\n",
       "0   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "2   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "3   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "4   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "\n",
       "   zucchini  word_count  neg_sent  neu_sent  pos_sent  comp_sent  \n",
       "0         0           4     0.535     0.465      0.00    -0.3182  \n",
       "1         0          20     0.000     1.000      0.00     0.0000  \n",
       "2         0           4     0.000     1.000      0.00     0.0000  \n",
       "3         0          35     0.070     0.930      0.00    -0.2755  \n",
       "4         0          18     0.120     0.489      0.39     0.7249  \n",
       "\n",
       "[5 rows x 10004 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize our bag of words transformers\n",
    "cvec = CountVectorizer(ngram_range=(1,5),      # check groups of up to 5 consecutive words\n",
    "                       stop_words=stops,       # set our ignored words\n",
    "                       max_features=10_000,    # limit the total nubmer of features to 10,000\n",
    "                       strip_accents='ascii',  # strip special accents of characters and force them to base ascii to prevent duplicates of the same word in different fonts\n",
    "                       min_df=3)               # declare a minimum number of appearances for a word to be considered\n",
    "\n",
    "# declare our X and y variables\n",
    "X = data.drop('subreddit', axis=1)\n",
    "y = data['subreddit']\n",
    "\n",
    "# transform our comments into a new dataframe\n",
    "Z = cvec.fit_transform(X['comment'])\n",
    "Z_fit = pd.DataFrame(Z.todense().astype('uint8'), columns= cvec.get_feature_names())\n",
    "\n",
    "# add back in our word count and sentiment score coclumns\n",
    "Z_fit[X.columns] = X\n",
    "Z_fit.drop('comment', axis=1, inplace=True)\n",
    "\n",
    "# check to make sure everything combined correctly\n",
    "Z_fit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f557794f-d89a-4fd6-aa80-e259926bb187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zikr</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>word_count</th>\n",
       "      <th>neg_sent</th>\n",
       "      <th>neu_sent</th>\n",
       "      <th>pos_sent</th>\n",
       "      <th>comp_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.8648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.3004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.4310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.7351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 10004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  01  02  03  04  05  07  08  10  ...  zeros  zikr  zone  zoomed  \\\n",
       "1159   0    0   0   0   0   0   0   0   0   1  ...      0     0     0       0   \n",
       "1160   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1161   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1162   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1163   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1164   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1165   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1166   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1167   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1168   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "1169   0    0   0   0   0   0   0   0   0   0  ...      0     0     0       0   \n",
       "\n",
       "      zucchini  word_count  neg_sent  neu_sent  pos_sent  comp_sent  \n",
       "1159         0          23     0.000     0.847     0.153     0.5859  \n",
       "1160         0          46     0.000     0.785     0.215     0.8648  \n",
       "1161         0           6     0.000     1.000     0.000     0.0000  \n",
       "1162         0          90     0.074     0.797     0.129     0.7269  \n",
       "1163         0           1     0.000     1.000     0.000     0.0000  \n",
       "1164         0          13     0.199     0.678     0.123    -0.3004  \n",
       "1165         0           3     0.000     0.476     0.524     0.2960  \n",
       "1166         0           9     0.000     0.778     0.222     0.4310  \n",
       "1167         0           5     0.160     0.840     0.000    -0.2732  \n",
       "1168         0           3     0.000     0.328     0.672     0.6249  \n",
       "1169         0          41     0.000     0.812     0.188     0.7351  \n",
       "\n",
       "[11 rows x 10004 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding an instance of a non-zero value in our data for use in presentation slides\n",
    "Z_fit[1159:1170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d48e8fa6-27eb-4047-9dc9-dff316926e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after transformation: (63388, 10004)\n",
      "Null count after transformation: 0\n"
     ]
    }
   ],
   "source": [
    "# check our shape and make sure we still dont have any nulls before modeling\n",
    "print(f'Shape after transformation: {Z_fit.shape}')\n",
    "print(f'Null count after transformation: {Z_fit.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f5d67-1878-4ea7-922a-c0023a52d239",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "Lets go ahead and split our data into training and testing sets and see if we can get some decent models running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe31d46-191f-4a0e-8fd6-443771cac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our split for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z_fit,y,random_state=413,train_size=.67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c0071-e5d0-4326-b638-c28654418d55",
   "metadata": {},
   "source": [
    "## !!QUICK DISCLAIMER HERE!!\n",
    "These models were fit on a machine using 32 GB of DDR4 3200 MHZ RAM, a Ryzen 3900X 12-core processor and an RTX 3060ti GPU with cuda cores enabled. Proceed with caution before running the below code as this still took several minutes to run. If you do not have a machine with cuda cores enabled please remove the ``` tree_method=\"gpu_hist\" ``` from the ```xg_model = xg.sklearn.XGBClassifier(tree_method=\"gpu_hist\")``` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2aa0bfe-37f1-477f-b830-34ef099c0014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8345145871105983\n",
      "Testing Accuracy: 0.7581146326306228\n",
      "F1 Score: 0.7680176049880799\n",
      "Recall Score: 0.7979422692197771\n",
      "Precision Score: 0.7402562969509501\n"
     ]
    }
   ],
   "source": [
    "# # initialize, fit, and make predictions with a logistic regression model\n",
    "# logreg = LogisticRegression(max_iter=10_000)\n",
    "# logreg.fit(X_train,y_train)\n",
    "# log_preds = logreg.predict(X_test)\n",
    "\n",
    "# # print our scores\n",
    "# print(f'Training Accuracy: {logreg.score(X_train, y_train)}')\n",
    "# print(f'Testing Accuracy: {logreg.score(X_test, y_test)}')\n",
    "# print(f'F1 Score: {f1_score(y_test, log_preds)}')\n",
    "# print(f'Recall Score: {recall_score(y_test, log_preds)}')\n",
    "# print(f'Precision Score: {precision_score(y_test, log_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e73262-ebc2-421e-bf4d-eec8537c4fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>net</td>\n",
       "      <td>-3.104940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7109</th>\n",
       "      <td>refilling</td>\n",
       "      <td>-3.086177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>passphrase</td>\n",
       "      <td>-2.938167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>magician</td>\n",
       "      <td>-2.748970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>refers</td>\n",
       "      <td>-2.728523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6979</th>\n",
       "      <td>rave</td>\n",
       "      <td>2.674706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6127</th>\n",
       "      <td>osha</td>\n",
       "      <td>2.768829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610</th>\n",
       "      <td>wedding</td>\n",
       "      <td>2.939022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9609</th>\n",
       "      <td>websites</td>\n",
       "      <td>3.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>saves</td>\n",
       "      <td>3.580372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3519 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Coefficient\n",
       "5752         net    -3.104940\n",
       "7109   refilling    -3.086177\n",
       "6268  passphrase    -2.938167\n",
       "5203    magician    -2.748970\n",
       "7100      refers    -2.728523\n",
       "...          ...          ...\n",
       "6979        rave     2.674706\n",
       "6127        osha     2.768829\n",
       "9610     wedding     2.939022\n",
       "9609    websites     3.003101\n",
       "7537       saves     3.580372\n",
       "\n",
       "[3519 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # check our top coefficients\n",
    "# log_coefficients = pd.DataFrame(zip(cvec.get_feature_names(),logreg.coef_[0]),columns=['Word','Coefficient']).sort_values(by = 'Coefficient')\n",
    "# log_coefficients[abs(log_coefficients['Coefficient']) > .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0713b721-da46-4ecf-957f-410702048062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8253078716240081\n",
      "Testing Accuracy: 0.7192026387494622\n",
      "F1 Score: 0.7340155768882448\n",
      "Recall Score: 0.7721253691530914\n",
      "Precision Score: 0.6994908086648831\n"
     ]
    }
   ],
   "source": [
    "# # initialize, fit, and make predictions with a random forrest model\n",
    "# forrest = RandomForestClassifier(max_features=100,max_depth=100)\n",
    "# forrest.fit(X_train,y_train)\n",
    "# forrest_preds = forrest.predict(X_test)\n",
    "\n",
    "# # print our scores\n",
    "# print(f'Training Accuracy: {forrest.score(X_train, y_train)}')\n",
    "# print(f'Testing Accuracy: {forrest.score(X_test, y_test)}')\n",
    "# print(f'F1 Score: {f1_score(y_test, forrest_preds)}')\n",
    "# print(f'Recall Score: {recall_score(y_test, forrest_preds)}')\n",
    "# print(f'Precision Score: {precision_score(y_test, forrest_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82f05b08-1ebe-49b0-bc8e-4ead6ed3e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training Accuracy: 0.7459087805222633\n",
      "Testing Accuracy: 0.7215450069314977\n",
      "F1 Score: 0.7378133861457442\n",
      "Recall Score: 0.7807945127179194\n",
      "Precision Score: 0.6993174061433447\n"
     ]
    }
   ],
   "source": [
    "# # initialize, fit, and make predictions with an XGBoot model\n",
    "# xg_model = xg.sklearn.XGBClassifier(tree_method=\"gpu_hist\")\n",
    "# xg_model.fit(X_train,y_train)\n",
    "# xg_preds = xg_model.predict(X_test)\n",
    "\n",
    "# print(f'Training Accuracy: {xg_model.score(X_train, y_train)}')\n",
    "# print(f'Testing Accuracy: {xg_model.score(X_test, y_test)}')\n",
    "# print(f'F1 Score: {f1_score(y_test, xg_preds)}')\n",
    "# print(f'Recall Score: {recall_score(y_test, xg_preds)}')\n",
    "# print(f'Precision Score: {precision_score(y_test, xg_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dae6f4-030f-453d-94f7-5e9a343ce998",
   "metadata": {},
   "source": [
    "# Results\n",
    "It looks like our top contender was logistic regression. It's still overfit but with more time we could revisit the stop words to try and remove words that are highly common in both subreddits. Some manual hyperparameter adjustments were made but gridsearching was avoided due to hardware and time constraints. XGBoost had the best fit overall and I'd be most interested to try and explore more using that model. Additionally it would be worth trying to use a few other modeling techniques such as SVMs or other types of tree methods. \n",
    "\n",
    "## Final Thoughts\n",
    "It's very clear that there is some clear divide between the types of comments on each subreddit but it would take more time and research to truely home in on a great model that could accurately distinguish between the two. We still managed to score well above our baseline score but I think some more work needs to be done here before this model could become super useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ba005-c6df-4386-8a80-90e44f94f0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
