{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c47b5bc-5e64-4ffb-b89e-c325f1ea4d9f",
   "metadata": {},
   "source": [
    "# Can we determine if a reddit user found a post satisfying or infuriating?\n",
    "## More specifically can we determine if a user posted their comment on r/oddlysatisfying or r/mildlyinfuriating using NLP?\n",
    "The goal of this project is to try and create a machine learning algorithm that can ready bulks of comment data on reddit and classify the subreddit from which that data came from. This project will only be looking at the two previously mentioned subreddits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920666a-1f9d-4975-8677-47eb6a472f55",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "Here we will be using the Pushshift.io Reddit API to collect comments from reddit posted on r/mildlyinfuriating and on r/oddlysatisfying. We'll be shooting for about 100,000 comments for each subreddit so that even after cleaning we will still have a sizable chunk of data to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2894621d-0950-449d-a02f-2447ff2f2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dee299-d957-44fe-b834-9d27f4dfc146",
   "metadata": {},
   "source": [
    "I'll be using two different functions here for collection. One function will handle individual requests to the pushshift servers for batches of 100 comments at a time. The other will set up a loop to gather and append each batch to a dataframe until we've reached the desired amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21b2d4f-24a2-428b-8e3b-ba262092a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gets a collection of comments harvested from our desired subreddit using the pushshift api\n",
    "def get_reddit_comment_data(subreddit, size = 100, before = None):\n",
    "    \n",
    "    # set our url and parameters of data we're collecting\n",
    "    url = 'https://api.pushshift.io/reddit/search/comment'\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size' : size,\n",
    "        'before' : before\n",
    "    }\n",
    "    \n",
    "    # connect to the page and collect the data returned\n",
    "    res = requests.get(url, params)\n",
    "    data = res.json()\n",
    "    \n",
    "    #convert our data into a DataFrame\n",
    "    df = pd.DataFrame(data['data'])\n",
    "    \n",
    "    # return relevant columns from our dataframe\n",
    "    return df[['author','body','created_utc','subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81620ec-0c33-4a1a-9f0a-c9449991f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function makes multiple requests successively so we can exceed the 100 comment limit and appends each request to a dataframe\n",
    "def get_bulk_reddit_comment_data(sub, loop = 10, stamp = None):\n",
    "    \n",
    "    print(f'Starting collection of {loop*100} comments from r/{sub}')\n",
    "    print(f'This will take at least {int(loop*2)} seconds')\n",
    "    \n",
    "    #retrieve the first batch of comments starting with the most recent post if no start time is specified\n",
    "    bulk_df = get_reddit_comment_data(subreddit = sub, before = stamp)\n",
    "    \n",
    "    # retrieve the timestamp of the oldest comment in the batch which will always be the first in the dataframe\n",
    "    timestamp = bulk_df['created_utc'].min()\n",
    "    \n",
    "    # grab the other 9 (or loop-1) sets of data requested\n",
    "    for i in range(loop-1):\n",
    "        \n",
    "        # grab the next most recent batch of comments based on our timestamp from the previous batch\n",
    "        temp_df = get_reddit_comment_data(subreddit = sub, before = timestamp)\n",
    "        \n",
    "        # set the new oldest comment timestamp based on the batch we just collected\n",
    "        timestamp = temp_df['created_utc'].min()\n",
    "        \n",
    "        # append our new batch of comments to the master dataframe\n",
    "        bulk_df = pd.concat([bulk_df,temp_df], axis=0)\n",
    "        \n",
    "        # wait 2 seconds before next loop to be polite to the pushshift servers\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # progress bar since this can take a while to run with the baked in wait between loops\n",
    "        print(f'{sub} progress is {int((i+1)/loop*100)}%', end='\\r', flush=True)\n",
    "    \n",
    "    # final print statement when loop completes to let us know\n",
    "    print(f'r/{sub} data collection complete! Size is {bulk_df.shape}')\n",
    "    \n",
    "    return bulk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9297da6-105f-49d5-b09f-cc8f346925c5",
   "metadata": {},
   "source": [
    "Now that we have the tools to collect, let's get a bunch of data! This can take a while with a 2 second delay per batch so I included a little progress bar to check in on it (1,000 loops for 2 different subredits will take over an hour).\n",
    "\n",
    "Original data gathered is included but used the same lines of code as below. These will demonstrate how the progress bar worked if you want to try them yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a79a8c3-bb3d-4c2e-9d87-994ad9bfebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection of 1000 comments from r/mildlyinfuriating\n",
      "This will take at least 20 seconds\n",
      "r/mildlyinfuriating data collection complete! Size is (1000, 4)\n",
      "Starting collection of 1000 comments from r/oddlysatisfying\n",
      "This will take at least 20 seconds\n",
      "r/oddlysatisfying data collection complete! Size is (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "# get comments from r/mildlyinfuriating and store them for cleaning\n",
    "mild_df = get_bulk_reddit_comment_data(sub = 'mildlyinfuriating', loop=10)\n",
    "mild_df.to_csv('../data/mildlyinfuriating_example.csv', index=False)\n",
    "\n",
    "# get comments from r/oddlysatisfying and store them for cleaning\n",
    "odd_df = get_bulk_reddit_comment_data(sub = 'oddlysatisfying', loop=10)\n",
    "odd_df.to_csv('../data/oddlysatisfying_example.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b9c26-a182-4afd-a168-255dd9a5854c",
   "metadata": {},
   "source": [
    "Now we can send out our collected comments for data cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9e70d-87f0-47bf-9ca5-94a83cba0d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
